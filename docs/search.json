[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Distill logs",
    "section": "",
    "text": "Distilling things that interest me\n\n\n\n\n\n\n\n\n\n\nOn writing: why is it hard to do?\n\n\n\n\n\n\n\n\nMay 28, 2025\n\n\n\n\n\n\n\nBasics of Exploratory Data Analysis\n\n\n\n\n\n\n\n\nMay 9, 2025\n\n\n\n\n\n\n\nStand out, produce stuff, build in the open\n\n\n\n\n\n\n\n\nApr 24, 2025\n\n\n\n\n\n\n\nCommitting once again!\n\n\n\n\n\n\n\n\nNov 28, 2020\n\n\n\n\n\n\n\nTerminal commands\n\n\n\n\n\n\n\n\nJun 2, 2020\n\n\n\n\n\n\n\nMy running journey\n\n\n\n\n\n\n\n\nFeb 20, 2020\n\n\n\n\n\n\n\nMy foray to overseas MS (Aug 2016 - present)\n\n\n\n\n\n\n\n\nJan 29, 2020\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-05-28-tough-to-write.html",
    "href": "posts/2025-05-28-tough-to-write.html",
    "title": "On writing: why is it hard to do?",
    "section": "",
    "text": "Why I loose urge to write, almost everytime.\nI have been thinking a lot, about writing of any topic that comes into my mind for a quick few paras. But I just end up adding those ideas as backlogs to my mind or note app. On thinking more about it, why I push the urge to write for “later”, and then ending up to dump the idea altogether, here are the findings: 1. When the urge occurs, it says that I can surely write a lot about it. Because it feels like I have a lot of things of say about the topic. 2. Also the thought that I can write a lot on a matter, makes me think that I will have to think a lot now to just write the first draft, than re-read all to ensure I sound intelligent. 3. That all sounds a big task, so I ditch it at the moment and end up loosing the urge altogether.\nSo I think here is the plan: 1. Just pick up the keyboard or a piece of paper, and write the thoughts as crisp and short as possible. 2. Start it with just being a couple of sentences maybe. 3. If I feel good doing so, maybe stretch it in the second sitting."
  },
  {
    "objectID": "posts/eda-calories-prediction-data.html",
    "href": "posts/eda-calories-prediction-data.html",
    "title": "Basics of Exploratory Data Analysis",
    "section": "",
    "text": "Loading the data\n\nThis data is from the Playground series in Kaggle (Season 5, Episode 5)\n\nCalorie Prediction Competition\n\nWe let the id column in csv files to be the indexes in the dataframe, else it would have become an additional and unnecessary feature to handle\n\n\ntrain_df = pd.read_csv('playground-series-s5e5/train.csv', index_col='id')\ntest_df = pd.read_csv('playground-series-s5e5/test.csv', index_col='id')\noriginal_df = pd.read_csv('playground-series-s5e5/calories.csv', index_col=\"User_ID\")\n\n\ntest_df.head()\n\n\n\n\n\n\n\n\nSex\nAge\nHeight\nWeight\nDuration\nHeart_Rate\nBody_Temp\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n750000\n0\n45\n177.0\n81.0\n7.0\n87.0\n39.8\n\n\n750001\n0\n26\n200.0\n97.0\n20.0\n101.0\n40.5\n\n\n750002\n1\n29\n188.0\n85.0\n16.0\n102.0\n40.4\n\n\n750003\n1\n39\n172.0\n73.0\n20.0\n107.0\n40.6\n\n\n750004\n1\n30\n173.0\n67.0\n16.0\n94.0\n40.5\n\n\n\n\n\n\n\n\n\nUnderstanding the data\n\nBelow we check if any Missing or NaN values present in either ‘train’, ‘test’, ‘original’ data\n\n\n# train_df.isna().sum().eq(0).all()\nprint((train_df.isna().sum() == 0).all())\nprint((test_df.isna().sum() == 0).all())\n\nTrue\nTrue\n\n\n\nGetting information on all features in the data, particularly we can find which columns are numerical and which are not\n\nWe identify that by looking at the Dtype for each column\n\nObject: Categorical / text or string\nfloat/int: is numerical, discrete or continuous\n\nThis data looks fairly straight forward, as mostly the features seem numerical\nOnly Sex feature is categorical, but that is also easy to handle as it has only 2 unique values\n\nThe stats of all numerical columns in train & test data look similar\n\nIf you try comparing any statistic for a column in both train & test, they are very close\nSo we can think of these data coming in from same distribution\n\n\n\nprint(train_df.describe().T)\nprint('--- ' * 20)\nprint('--- ' * 20)\nprint(test_df.describe().T)\n\n               count        mean        std    min    25%    50%    75%    max\nAge         750000.0   41.420404  15.175049   20.0   28.0   40.0   52.0   79.0\nHeight      750000.0  174.697685  12.824496  126.0  164.0  174.0  185.0  222.0\nWeight      750000.0   75.145668  13.982704   36.0   63.0   74.0   87.0  132.0\nDuration    750000.0   15.421015   8.354095    1.0    8.0   15.0   23.0   30.0\nHeart_Rate  750000.0   95.483995   9.449845   67.0   88.0   95.0  103.0  128.0\nBody_Temp   750000.0   40.036253   0.779875   37.1   39.6   40.3   40.7   41.5\nCalories    750000.0   88.282781  62.395349    1.0   34.0   77.0  136.0  314.0\n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- \n--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- \n               count        mean        std    min    25%    50%    75%    max\nAge         250000.0   41.452464  15.177769   20.0   28.0   40.0   52.0   79.0\nHeight      250000.0  174.725624  12.822039  127.0  164.0  174.0  185.0  219.0\nWeight      250000.0   75.147712  13.979513   39.0   63.0   74.0   87.0  126.0\nDuration    250000.0   15.415428   8.349133    1.0    8.0   15.0   23.0   30.0\nHeart_Rate  250000.0   95.479084   9.450161   67.0   88.0   95.0  103.0  128.0\nBody_Temp   250000.0   40.036093   0.778448   37.1   39.6   40.3   40.6   41.5\n\n\n\nGet column names seperated into numerical & categorical categories\n\n\nnumerical_columns = [col for col in train_df.columns if train_df[col].dtype != 'object']\ncategorical_columns = [col for col in train_df.columns if train_df[col].dtype == 'object']\n\n\n\nUnivariate analysis: Plotting the data\n\n1. Plot the categorical features, observations from below plots:\n\nWell balanced male / female classes in the train data\nWhen plotting box-plt of sex vs calories; min/max, all quartiles are located similarly for both male and female classes. Although there are a few more outliers calories in male class as compared to female class.\n\n\nfor col in categorical_columns:\n    plt.figure(figsize=(12,6))\n\n    plt.subplot(1,2,1)\n    sns.boxplot(x=train_df[col], y=train_df['Calories'])\n    \n    plt.subplot(1,2,2)\n    counts = train_df[col].value_counts()\n    plt.pie(counts, labels=counts.index, autopct='%1.1f%%')\n\n\n\n\n\n\n\n\n\ntrain_df[numerical_columns].skew()\n\nAge           0.436397\nHeight        0.051777\nWeight        0.211194\nDuration      0.026259\nHeart_Rate   -0.005668\nBody_Temp    -1.022361\nCalories      0.539196\ndtype: float64\n\n\n\n\n\nimage.png\n\n\n\n\n2. Plotting and observe the numerical features:\n\nAge:\n\nheavily skewed to right\nfrequency decrease with increase in age\nmay suggest that mostly younger people using the workout monitoring app\n\nHeight:\n\nminimal skew\nsymmetrical, bell shaped, approximately Gaussian (Normal) distribution\n\nWeight:\n\nslightly right skewed, which represent real-world data, upper limits can vary for wegihts\n\nDuration:\n\napproximately Uniform distribution\n\nHeart_rate:\n\nNormally distributed with slight righ skew\nsmall portion of people with elevated heart rates\n\nBody_temp:\n\nNegatively skewed\nafter workout, slightly elevated from normal temperatures makes sense\n\nCalories:\n\nRight skewed, most people burn fewer calories per session, small fraction burn significantly more\n\n\n\nfor col in numerical_columns:\n    plt.figure(figsize=(18,6))\n\n    plt.subplot(1,3,1)\n    sns.histplot(data=train_df[col], bins=30, kde=True)\n\n    plt.subplot(1,3,2)\n    sns.violinplot(x=train_df[col])\n\n    plt.subplot(1,3,3)\n    sns.boxplot(data=train_df[col], orient='h')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBivariate analysis\n\nLet’s find the Correlations between the numerical features\n\n\n# train_df[numerical_columns].corr().style.background_gradient()\ncorr = train_df[numerical_columns].corr()\ncorr\n\n\n\n\n\n\n\n\nAge\nHeight\nWeight\nDuration\nHeart_Rate\nBody_Temp\nCalories\n\n\n\n\nAge\n1.000000\n0.011975\n0.073690\n0.015656\n0.017037\n0.030275\n0.145683\n\n\nHeight\n0.011975\n1.000000\n0.957967\n-0.029936\n-0.013234\n-0.034641\n-0.004026\n\n\nWeight\n0.073690\n0.957967\n1.000000\n-0.020845\n-0.002384\n-0.023717\n0.015863\n\n\nDuration\n0.015656\n-0.029936\n-0.020845\n1.000000\n0.875327\n0.903066\n0.959908\n\n\nHeart_Rate\n0.017037\n-0.013234\n-0.002384\n0.875327\n1.000000\n0.795972\n0.908748\n\n\nBody_Temp\n0.030275\n-0.034641\n-0.023717\n0.903066\n0.795972\n1.000000\n0.828671\n\n\nCalories\n0.145683\n-0.004026\n0.015863\n0.959908\n0.908748\n0.828671\n1.000000\n\n\n\n\n\n\n\n\n# Plot the correlation matrix with colors\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nplt.figure(figsize=(5, 10))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\n\n\n\n\n\n\n\n\nQuite clear from above color gradients:\n\nCalories is highly correlated with Duration, Heart_Rate, Body_Temp\nHeart_Rate and Body_Temp are highly correlated\nDuration and Heart_Rate are highly correlated\nDuration and Body_Temp are highly correlated\n\n\n\nScatter plots amongst the numerical features\n\nHardly any patterns emerge when we compare Calories to Age, Weight, Height\nWhereas, strong patterns can be seen with Duration, Heart_Rate, Body_Temp\n\n\nSo it’s fair to say that scatter plots confirm the correlation numbers\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\naxes = axes.ravel()  # Flatten the 2D array of axes for easy iteration\n\ntarget = 'Calories'\nfor i, feature in enumerate(numerical_columns[:-1]):\n    axes[i].scatter(train_df[feature], train_df[target], alpha=0.2)\n    axes[i].set_xlabel(feature)\n    axes[i].set_ylabel(target)\n    axes[i].set_title(f'{feature} vs {target}')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nData preprocessing\n\nMap the Sex feature into numerical\n\n\ntrain_df['Sex'] = train_df['Sex'].map({'male':0, 'female':1})\ntest_df['Sex'] = test_df['Sex'].map({'male':0, 'female':1})\n\n\nMake X and y variables\n\n\nX = train_df.drop('Calories', axis=1)\ny = np.log1p(train_df['Calories'])\ny_scale = (train_df['Calories'] - train_df['Calories'].mean()) / train_df['Calories'].std()\n\nX_test = test_df\n\n\nNow let’s try to differentiate between log transformation (y) and standard scaled (y_scale)\n\nObservations from the boxplots and histplots below:\n\nScaling just scales the feature, but the distributions remains same, skew remains the same, outliers are still outliers\nLog transform changes the distribution by reducing the skew, also outliers are compressed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# renaming Gender in original_df to Sex\n\noriginal_df['Gender'] = original_df['Gender'].map({'male':0, 'female':1})\noriginal_df = original_df.rename(columns={'Gender': 'Sex'})\n\n\nAs now we have original_df with same column names and no null samples, we can merge the train_df with original_df\n\n\n# merge the original data to train_df\ntrain_df = pd.concat([train_df, original_df])\n\n(765000, 8)\n\n\n\nThis way we can get the Mutual Information scores of every feature in X against y (target)\n\n\nfrom sklearn.feature_selection import mutual_info_regression\nmi = mutual_info_regression(X=X, y=y, n_neighbors=5)\nmutual_info = pd.Series(mi)\nmutual_info.index = X.columns\n\n\nThe results of mututal_info_regression clearly concurs the scatter plot patterns and correlations\n\n\nDuration, Heart_Rate, Body_Temp are containing most mutual information with the Calories\nBut, then, by human understanding, Duration also positively impacts both Heart_Rate & Body_Temp\n\n\nmutual_info = pd.DataFrame(mutual_info.sort_values(ascending=False), columns=['Mutual Information'])\nmutual_info\n\n\n\n\n\n\n\n\nMutual Information\n\n\n\n\nDuration\n1.641224\n\n\nBody_Temp\n1.120950\n\n\nHeart_Rate\n0.976502\n\n\nAge\n0.097669\n\n\nHeight\n0.055549\n\n\nWeight\n0.055448\n\n\nSex\n0.016135\n\n\n\n\n\n\n\n\nSplit the merged df, into train, dev, test split\n\n\n\n# shuffle\ntrain_df = train_df.sample(frac=1)\n\n\ndef scale_numerical_feature(X):\n    return (X - np.mean(X)) / np.std(X)\n\n\n\nBasic models\n\nSimple Linear regression using just most informative of the features\n\nI want to see how using just Duration would predict the Calories\n\n\n\nclass SimpleLinearRegression:\n    def __init__(self, lr = 0.01):\n        self.lr = lr\n        self.w = np.random.rand() * 0.1\n        self.b = np.random.rand() * 0\n    \n    def get_params(self):\n        return (self.w, self.b)\n    \n    # compute predictions for the given input X\n    def forward(self, X):\n        return (self.w * X + self.b)\n\n    def fit(self, X, y, epochs=100):\n        n = len(X)\n        for epoch in range(epochs):\n            # calculate prediction for X\n            y_pred = self.forward(X)\n\n            # calculate loss wrt y\n            loss = np.mean((y_pred - y)**2)\n\n            # calculate gradients\n            dw = (2/n) * np.sum((y_pred - y) * X)\n            db = (2/n) * np.sum(y_pred - y)\n\n            # update the params\n            self.w -= self.lr * dw\n            self.b -= self.lr * db\n\n            # print(f'loss={loss}')\n            if (epoch + 1) % 100 == 0 or epoch == 0:\n                print(f'Epoch {epoch + 1}: Loss {loss:.4f}')\n    \n    def evaluate(self, X, y):\n        y_pred = self.forward(X)\n        mse = np.mean((y_pred - y) ** 2)\n        rmse = np.sqrt(mse)\n        rmsle = np.sqrt(np.mean((np.log1p(y) - np.log1p(y_pred)) ** 2))\n        return mse, rmse, rmsle\n\n\nn1 = int(0.8 * len(train_df))\nn2 = int(0.9 * len(train_df))\nX = scale_numerical_feature(train_df['Heart_Rate']) #scale_numerical_feature(train_df['Duration'])\n# y = train_df['Calories']\ny = np.log1p(train_df['Calories'])\n\nX_train, y_train = X.values[:n1], y.values[:n1]\nX_dev, y_dev = X.values[n1:n2], y.values[n1:n2]\nX_test, y_test = X.values[n2:], y.values[n2:]\n\n\nmodel = SimpleLinearRegression(lr=.1)\nmodel.fit(X_train, y_train, epochs=100)\n\nEpoch 1: Loss 18.0598\nEpoch 100: Loss 0.2066\n\n\n\nSome notes from training the SimpleLinearRegression using Duration to explain Calories:\n\nWe initialied the self.w as a scalar, so must use only one feature from the dataframe for modeling.\nWhen using the Duration without any centering to 0, it starts with very high loss\n\nand explodes very fast for lr = 1e-2\nbut at lr = 1e-3, loss quickly falls around 1\n\nWith Duration scaled around 0, but Calories not transformed with np.log1p, again loss starts very high and goes down slow\nWith Duration - scaled, and Calories log1p transformed, the loss is very slow to converge. My observations are below:\n\nYour gradients are working ✅\nBut learning rate is too low, or\nYour input/target scales are still not ideal, or\nThe model’s capacity (just 1 feature) may be too limited for the data variance\n\n\nLet’s work on comparing our custom linear regression model with the standard Sklearn LR\n\n\nfrom sklearn.linear_model import LinearRegression\n\n\nsk_model = LinearRegression()\nsk_model.fit(X_train.reshape(-1,1), y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\ny_pred = sk_model.predict(X_dev.reshape(-1,1))\nrmse = np.sqrt(np.mean((y_dev - y_pred) ** 2))\nrmse\n\nnp.float64(0.4533044214500241)\n\n\n\n\nWeight converged to, by sklearn model: 0.8495730558448487\nWeight converged to, by custom model:   0.8495730556549425\n\n\n\n\nbias converged to, by sklearn model: 4.1413464025473585\nbias converged to, by custom model:   4.141346401700354\n\n\n\nprint(f'weights reached at from two methods are same? {math.isclose(sk_model.coef_[0], model.get_params()[0])}')\nprint(f'biases reached at from two methods are same? {math.isclose(sk_model.intercept_, model.get_params()[1])}')\n\nweights reached at from two methods are same? True\nbiases reached at from two methods are same? True\n\n\n\nGood to conclude that we reach at almost same weight and bias as sklearn LR\n\nFor modeling Calories only on Heart_Rate"
  },
  {
    "objectID": "posts/2020-01-29-My_foray.html",
    "href": "posts/2020-01-29-My_foray.html",
    "title": "My foray to overseas MS (Aug 2016 - present)",
    "section": "",
    "text": "As mush as I cherish getting this opportunity, there have been bitter and sweet memories during these 3 years. A novel though that just flew by at 4:24 AM made me to sit and take some quick notes.\nMy Nanaji passed away on Nov 10th, 2016 so it’s been 3+ years to that and I understood life doesn’t stop. My mother passed away on June 6th 2019 and it’s been 6 months to that already, again, life doesn’t stop. I pledge to myself today while remembering them both, I will tirelessly work for my development here onwards.\nHere are some learnings I had while studying, working in the US and again back to India.\n\nFriendships are hard to come by - In these 3 years I came across many people, only one became long term friend who is to stay.\nRegret of not being to spend time with loved ones is killing - I would add it to top in the life goals list. No matter how adventurous things I achieve in life, I’ll regret not fulfilling this one.\nI would again love to do it, go out in the unknown territory, explore the world and build a new life - only this time with lot informed decisions.\nIt’s always worth taking risks, especially the one where you’re uncertain of the end result. There is no fun in embarking of a journey if you’re familiar with its entirety.\nIn my thoughts I have always been true to myself. But have not done much to work on my shortcomings. I want to do well in Machine Learning field but haven’t been practicing much. I lack heavily on hands on experience, okay, here’s the truth in blunt words. I’ve got my first job in N-Join without much hands on, then got another with Ai Reverie based on recommendation from N-Join (again very less hands on or useful work), the worked for Fractal analytics on completely out of my interest SQL work (although I learned many other things here). Now I am working for Samsung R&D India (supposedly on DL) but again it’s just floating on the water. There is no deep dive, am doing nothing for my development in this perspective.\n\nSo I COMMIT today to work hard on this front of my life. Additionally, things I want to explore and excel into are: - Kaggle competitions - Upload my work to Github - Video creation of my work, things I love (maybe Vlog, blog etc.) - Get back to SPORTS, be fully fit throughout 2020 and achieve best physical shape - Read for 30 mins before sleeping. Complete 124 books in 2020."
  },
  {
    "objectID": "posts/2020-06-02-Terminal_Find_Grep.html",
    "href": "posts/2020-06-02-Terminal_Find_Grep.html",
    "title": "Terminal commands",
    "section": "",
    "text": "find . | Find all the files & directories under current directory\nfind &lt;dir&gt; | Find all the files & directories under\n\nfind &lt;dir&gt; -type d | Find only directories under specific directory\nfind . -type d | Find only directories under current directory\nfind &lt;dir&gt; -type f | Find only files under specific directory\nfind . -type f | Find only files under current directory"
  },
  {
    "objectID": "posts/2020-06-02-Terminal_Find_Grep.html#grep-global-regular-expression-print",
    "href": "posts/2020-06-02-Terminal_Find_Grep.html#grep-global-regular-expression-print",
    "title": "Terminal commands",
    "section": "_grep (Global Regular Expression Print)",
    "text": "_grep (Global Regular Expression Print)\n\ngrep is case sensitive\n\n\nFinding is a given text is present in some file\n\ngrep \"text_to_find\" &lt;file_name&gt; | Searching for some text in a normal file\ngrep -w \"text_to_find\" &lt;file_name&gt; | Return results from file_name only when whole words match text_to_find\ngrep -wi \"text_to_find\" &lt;file_name&gt; | Returns results with both lower case and upper case with text_to_find\n\n\n\nFinding some additional information\n\nLine number of where we found our match\n\nReturns results with line number.\n\n\ngrep -win \"text_to_find\" &lt;file_name&gt; | Returns results with line numbers in file_name\ngrep -win \"text_to_find\" ./* | Returns results with line numbers in all files in current directory + Will throw error for any subdirectory that might be present.\ngrep -win \"text_to_find\" ./*.txt | Doesn’t try to search in any subdirectory.\ngrep -winr \"text_to_find\" . | To search every file and through every subdirectory, a recursive search, might get lot of results.\n\n\nGetting some additional context of where this match is found, see a certain number of lines before and after a match.\n\n\ngrep -win -B 4 \"text_to_find\" &lt;file_name&gt; | 4 lines Before all of our match\ngrep -win -A 4 \"text_to_find\" &lt;file_name&gt; | 4 lines After all of our match\ngrep -win -C 2 \"text_to_find\" &lt;file_name&gt; | 2 lines Before and After all of our match\n\n\nIf you’re only interested in file_names with the matches, NOT in the matches themselves.\n\n\ngrep -wirl \"text_to_find\" . | Recursive result of all files with the match\ngrep -wirc \"text_to_find\" . | Recursive result of all files with the match + Number of matches in each file\n\n\n\n\nPipe the output of other commands in to grep to search for something\n\nhistory | grep \"git commit\"\nhistory | grep \"git commit\" | grep \"dotfile\"\n\n\ngrep uses Posix regular expressions by default\n\ngrep \"...-...-....\" &lt;filename&gt;\ngrep \"\\d{3}-\\d{3}-\\d{4}\" &lt;filename&gt; | This wouldn’t work, because this is pro compatible regular expressions which grep doesn’t use.\ngrep -P \"\\d{3}-\\d{3}-\\d{4}\" &lt;filename&gt; | This would allow it to work on Linux, NOT on Mac\ngrep -wirlP \"\\d{3}-\\d{3}-\\d{4}\" &lt;filename&gt; | Return recursive list of list with matching phone numbers\ngrep -V\n\nMac uses BSD grep\nLinux uses GNU grep`\n\nbrew install grep --with-default-names\n\n--with-default-names | It will install it as grep, else as ggrep (allowing us to use both BSD and GNU grep)"
  },
  {
    "objectID": "posts/2025-04-24-Restart.html",
    "href": "posts/2025-04-24-Restart.html",
    "title": "Stand out, produce stuff, build in the open",
    "section": "",
    "text": "Some background\nSince I last mentioned about my back injury in this blog, it’s been well over 5 years now, and they just flew by. In that period I’ve become a father to a daughter who just might be sweetest in the world, totaly failed at managing / healing my injury, got to a point physically that was my worst - fat and depressed, thought that am not built to do jobs and quit without having any side hustle or backup plan. It’s been an year since I quit the job, and wonder where that year went. Anyways, here I am at this juncture that urgency has hit back once more in my life and I must get an “unhealthy obsession” kinda routine to make me stand out in at least one direction, even if it’s getting back to a job.\n\n\nWhere I stand now\nI want to work at a very good tech company, likes of Google, Meta which would give me some validation and self satisfaction. Going to an mediocre company might be plausible at the current situation, but I strongly don’t want to get stuck there. But from what I perceive now, it’s impossible to get my profile pass the resume shortlisting at top companies, unless there is a aggressive hiring and HRs are reaching out themselves. But in that case I might easily get thrown out in initial couple of interviews.\n\n\nMaking myself stand out\nSo here is my plan to stand out so that recruiters or professionals appreciate the work I generate and recommend it towards good roles. No more working in the silo of following some training and writing a bunch of codes, having those bullshit repos on my laptop or maybe a private githup repo. I want to explain things in my blogs, expose myself to the world of vlogging while learning new stuff and in general growing. Also I think I can teach not just tech, but also handling back pain to some extent, although not in medical terms.\n\nWhat will I blog about:\n\nBasic steps of doing Exploratory Data Analysis\nHandling issues with structured data\nRegression algorithms - from scratch, and using libraries\nClassification algorithms - from scratch, and using libraries\nClustering algoritms - from scratch, and using libraries\n[Grow the list]\n\n\n\nWhat will I make videos about:\n\nRe-building back, what makes seating so painful for someone who already has a back problem\nFat loss, why can it be so easy\nStarting CALISTHENICS\nAbout coding algorithms from scratch\n\n\n\nSteps of execution:\n\nInitially I plan to post a blog every alternate day at least, at least uptil the topics are relatively trivial\nMaybe 2 videos every week, this is a tough one to target maybe.\n\nI want to post and update to this blog in a month time, to document my journey. My back is against a wall, so f*** it, let’s do it.\nCheers!"
  },
  {
    "objectID": "posts/2020-02-20-Running-transformation.html",
    "href": "posts/2020-02-20-Running-transformation.html",
    "title": "My running journey",
    "section": "",
    "text": "Let me admit, today’s date looks great 20-02-2020, although its not Palindrome, which was on 02-02-2020. Amazing isn’t it, two great looking date in same month.\n“I have never been a long distance runner, till 30 January 2020”, let me explain the statement. I have spent my school days playing all kind of sports ranging from Cricket, football to volleyball, basketball. So I have an assumption of my physique of someone as athletic. Although never had a very mascular physique like “athletes”. I passed out 12th standard in 2006 and never regularly engaged in sports or physical activities, except once in a while. Untill 2012, when my weight shot up to 80kg which is slight overweight for my height of 5’10”. This was the period when I started working and sunk into a comfortable lifestyle with friends and did not worry about health.\nThis period of weight gain brought me back to senses that I should do something about it, although all I did was running in teadmill for maximum 2 kilometers daily along with some stretching and abs workouts. Doing this for approximately 2 months I saw drastic weight loss of 7-8 kgs, which again threw me to my confortable zone and the cycle of workouts broke from 6-8 months, leading to weight gain again. Al this cycle continued for 2 more years, where I lost some pounds 3-4 times leading back myself to sedintary lifestyle and gaining back same weight. All this while the maximum I could run was 2 kms with humungous effort, most of which were on treadmills.\nI moved up a bit to 3-3.5 kms range last year while working in San Antonio, where I fell in love with trail running (Leon Creek trail). I reached my peak of putting 70 kms in month of May 2019 where most of the runs would touch at least 3 kms. Soon I moved back to India and with hell lot of happening in life I lost that running touch until January 2020.\nAlthough I restarted with my runs in October last year, they were highly sporadic and went a maximum of 2.5 kms. It wasn’t until 30th January, it was surely a magical day in the way I felt. Because I wasn’t thinking a lot about my speed and taking some brisk walk break during the run, there arose the power, will, excitement that I can push myself. I put 5.5 kms on my Strava that day, had run for this distance a couple of times in past, this was the very first time I was feeling comfortable and enjoying a run. This day gave me a unimaginable confidence that I have something and can push my self further. Since this day, I have run for 14 times in February 2020, 90 percent of these runs being 4-5 kms. Today, again I had some magical power and was feeling good enough to extend my long runs record to 10 kms. There was a certain confidence, confort while doing so.\n\n\n\nSnapshot of today’s run stats\n\n\nI know I have talked about this drastic progress in tems of being magical, but as we all know there is no magic, I must admit that most of the credit goes to my improved diet (this is my assumption for now, would update in future as more discoveries arise). Undoubtedly, getting into a diet of greens, fruits, fresh cooked food (entire credit goes to my wife) had transformative change in my runs. My shins feel less breaking, even though I start with tight calves, they soon recover"
  },
  {
    "objectID": "posts/2020-11-28-Committing_once_again.html",
    "href": "posts/2020-11-28-Committing_once_again.html",
    "title": "Committing once again!",
    "section": "",
    "text": "Well, it’s high time I document my current feelings and thoughts.\nLately my mind has been juggling several to-dos’. I see myself as someone who has good taste about a lot of things (which obviously can be debated upon), but I have never gone more than a couple of steps further from thinking about doing something. Few examples of things I’ve been thinking about regularly engaging myself with are: reading & writing regularly, meditating as per the technique from Naval, getting super fast with typing, coding basics of Python, guitar basics, blogging, YouTube, and above all achieving a fit and young body.\nSince I last wrote about my running transformation, I finished 200 kilometres in month of March 2020. Within a couple of weeks of achieving this personal best run record I did end up hurting my lower back. And ever since, which now have been painful 8 months, I have lived with excruciating pain and stiffness in lower back. These times have shown me getting pissed off on very silly things which I feel were part contributed by this “back” trouble. Of course there were times I experienced depression feelings, making me think hard on the how my future is going to be, to what lengths am I going to miss out on the fun of life. After my ignorance and utter wastage of many months, I did join a physiotherapy session for 2 weeks now.\nSo for past 2 weeks, my daily sessions at physio constitutes of 30 minutes of core strengthening and balancing exercises, 8 minutes of Laser/ultrasound, IFT, and heat therapies each. These weeks have brought to me a sense of strength in the lower back, and although not more than 20% recovered, I sense that I am on right track. These sessions have also introduced me to a great world of strength training using resistance bands (about which I plan on writing and recording in more detail as soon as I am ready to do so). I plan on improving my muscle strength, maintain body weight, and optimal flexibility so as to attain same agility and power I had in earlier of my life. Feeling is, that there are high chances of becoming even better than I have ever been physically, mentally, spiritually, emotionally, socially."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Pending item.."
  }
]